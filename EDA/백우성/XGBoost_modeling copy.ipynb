{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\com\\Documents\\GitHub\\level1-classificationinmachinelearning-recsys-06\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Dict\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Code 경로 추가\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(\"\"))))\n",
    "print(sys.path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 호출\n",
    "data_path: str = \"../../data\"\n",
    "## raw.csv가 없는 경우 실행\n",
    "# from Code.dataset.merge_all import merge_all\n",
    "# df = merge_all(data_path)\n",
    "df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"scaled_eda2.csv\"))\n",
    "submission_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, \"test.csv\"))  # ID, target 열만 가진 데이터 미리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE\n",
    "\n",
    "def get_over_sampler(x_train, y_train, strategy='ADASYN', sampling_strategy='auto'):\n",
    "    if strategy == 'SMOTE':\n",
    "        sampler = SMOTE(sampling_strategy=sampling_strategy)\n",
    "    elif strategy == 'BorderlineSMOTE':\n",
    "        sampler = BorderlineSMOTE(sampling_strategy=sampling_strategy)\n",
    "    elif strategy == 'ADASYN':\n",
    "        sampler = ADASYN(sampling_strategy=sampling_strategy)\n",
    "    elif strategy == 'SVMSMOTE':\n",
    "        sampler = SVMSMOTE(sampling_strategy=sampling_strategy)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    x_resampled, y_resampled = sampler.fit_resample(x_train, y_train)\n",
    "    return x_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', '_type', 'target', 'scaled_log_hashrate',\n",
       "       'scaled_log_open_interest', 'scaled_log_coinbase_premium_index',\n",
       "       'scaled_funding_rates', 'scaled_estimated_block_reward',\n",
       "       'scaled_liquidation_diff', 'scaled_log_total_liquidation',\n",
       "       'scaled_log_total_taker_volume', 'scaled_utxo_count',\n",
       "       'scaled_total_transactions_count', 'taker_buy_sell_ratio',\n",
       "       'moving_avg_scaled_log_total_volume', 'open_interest_diff',\n",
       "       'network_activity_ratio_diff', 'average_transaction_value_diff',\n",
       "       'network_load_diff', 'fee_burden_diff', 'market_pressure_diff',\n",
       "       'liquidation_risk_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feauture = pd.read_csv('new_feature.csv')\n",
    "new_df = pd.concat([df, new_feauture.drop(columns=['Unnamed: 0','ID'])], axis=1)\n",
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = new_df.drop(columns=[\n",
    "    'network_activity_ratio_diff',\n",
    "    #'average_transaction_value_diff',\n",
    "    #'network_load_diff',\n",
    "    'fee_burden_diff',\n",
    "    #'market_pressure_diff',\n",
    "    #'liquidation_risk_diff'\n",
    "    ],inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 부스팅 라운드 수: 275\n",
      "     train-mlogloss-mean  train-mlogloss-std  test-mlogloss-mean  \\\n",
      "270             0.892901            0.003722            1.136598   \n",
      "271             0.891928            0.003711            1.136278   \n",
      "272             0.890998            0.003705            1.136007   \n",
      "273             0.890115            0.003736            1.135770   \n",
      "274             0.889159            0.003706            1.135462   \n",
      "\n",
      "     test-mlogloss-std  \n",
      "270           0.010328  \n",
      "271           0.010333  \n",
      "272           0.010360  \n",
      "273           0.010366  \n",
      "274           0.010407  \n",
      "[0]\ttrain-mlogloss:1.38253\teval-mlogloss:1.38359\n",
      "[1]\ttrain-mlogloss:1.37885\teval-mlogloss:1.38078\n",
      "[2]\ttrain-mlogloss:1.37524\teval-mlogloss:1.37818\n",
      "[3]\ttrain-mlogloss:1.37162\teval-mlogloss:1.37551\n",
      "[4]\ttrain-mlogloss:1.36797\teval-mlogloss:1.37272\n",
      "[5]\ttrain-mlogloss:1.36441\teval-mlogloss:1.37005\n",
      "[6]\ttrain-mlogloss:1.36075\teval-mlogloss:1.36755\n",
      "[7]\ttrain-mlogloss:1.35740\teval-mlogloss:1.36512\n",
      "[8]\ttrain-mlogloss:1.35395\teval-mlogloss:1.36253\n",
      "[9]\ttrain-mlogloss:1.35037\teval-mlogloss:1.36016\n",
      "[10]\ttrain-mlogloss:1.34708\teval-mlogloss:1.35762\n",
      "[11]\ttrain-mlogloss:1.34369\teval-mlogloss:1.35525\n",
      "[12]\ttrain-mlogloss:1.34029\teval-mlogloss:1.35291\n",
      "[13]\ttrain-mlogloss:1.33698\teval-mlogloss:1.35049\n",
      "[14]\ttrain-mlogloss:1.33362\teval-mlogloss:1.34809\n",
      "[15]\ttrain-mlogloss:1.33018\teval-mlogloss:1.34581\n",
      "[16]\ttrain-mlogloss:1.32682\teval-mlogloss:1.34343\n",
      "[17]\ttrain-mlogloss:1.32385\teval-mlogloss:1.34124\n",
      "[18]\ttrain-mlogloss:1.32082\teval-mlogloss:1.33903\n",
      "[19]\ttrain-mlogloss:1.31767\teval-mlogloss:1.33687\n",
      "[20]\ttrain-mlogloss:1.31443\teval-mlogloss:1.33482\n",
      "[21]\ttrain-mlogloss:1.31117\teval-mlogloss:1.33271\n",
      "[22]\ttrain-mlogloss:1.30810\teval-mlogloss:1.33051\n",
      "[23]\ttrain-mlogloss:1.30508\teval-mlogloss:1.32851\n",
      "[24]\ttrain-mlogloss:1.30213\teval-mlogloss:1.32641\n",
      "[25]\ttrain-mlogloss:1.29917\teval-mlogloss:1.32439\n",
      "[26]\ttrain-mlogloss:1.29623\teval-mlogloss:1.32235\n",
      "[27]\ttrain-mlogloss:1.29330\teval-mlogloss:1.32025\n",
      "[28]\ttrain-mlogloss:1.29033\teval-mlogloss:1.31823\n",
      "[29]\ttrain-mlogloss:1.28744\teval-mlogloss:1.31636\n",
      "[30]\ttrain-mlogloss:1.28453\teval-mlogloss:1.31441\n",
      "[31]\ttrain-mlogloss:1.28157\teval-mlogloss:1.31253\n",
      "[32]\ttrain-mlogloss:1.27874\teval-mlogloss:1.31057\n",
      "[33]\ttrain-mlogloss:1.27605\teval-mlogloss:1.30874\n",
      "[34]\ttrain-mlogloss:1.27324\teval-mlogloss:1.30689\n",
      "[35]\ttrain-mlogloss:1.27048\teval-mlogloss:1.30500\n",
      "[36]\ttrain-mlogloss:1.26780\teval-mlogloss:1.30320\n",
      "[37]\ttrain-mlogloss:1.26519\teval-mlogloss:1.30148\n",
      "[38]\ttrain-mlogloss:1.26243\teval-mlogloss:1.29979\n",
      "[39]\ttrain-mlogloss:1.25971\teval-mlogloss:1.29803\n",
      "[40]\ttrain-mlogloss:1.25715\teval-mlogloss:1.29629\n",
      "[41]\ttrain-mlogloss:1.25452\teval-mlogloss:1.29454\n",
      "[42]\ttrain-mlogloss:1.25202\teval-mlogloss:1.29292\n",
      "[43]\ttrain-mlogloss:1.24952\teval-mlogloss:1.29126\n",
      "[44]\ttrain-mlogloss:1.24693\teval-mlogloss:1.28965\n",
      "[45]\ttrain-mlogloss:1.24435\teval-mlogloss:1.28796\n",
      "[46]\ttrain-mlogloss:1.24176\teval-mlogloss:1.28632\n",
      "[47]\ttrain-mlogloss:1.23925\teval-mlogloss:1.28479\n",
      "[48]\ttrain-mlogloss:1.23681\teval-mlogloss:1.28326\n",
      "[49]\ttrain-mlogloss:1.23436\teval-mlogloss:1.28165\n",
      "[50]\ttrain-mlogloss:1.23190\teval-mlogloss:1.28009\n",
      "[51]\ttrain-mlogloss:1.22952\teval-mlogloss:1.27853\n",
      "[52]\ttrain-mlogloss:1.22709\teval-mlogloss:1.27708\n",
      "[53]\ttrain-mlogloss:1.22469\teval-mlogloss:1.27557\n",
      "[54]\ttrain-mlogloss:1.22231\teval-mlogloss:1.27411\n",
      "[55]\ttrain-mlogloss:1.21985\teval-mlogloss:1.27265\n",
      "[56]\ttrain-mlogloss:1.21741\teval-mlogloss:1.27119\n",
      "[57]\ttrain-mlogloss:1.21506\teval-mlogloss:1.26962\n",
      "[58]\ttrain-mlogloss:1.21263\teval-mlogloss:1.26821\n",
      "[59]\ttrain-mlogloss:1.21031\teval-mlogloss:1.26685\n",
      "[60]\ttrain-mlogloss:1.20794\teval-mlogloss:1.26551\n",
      "[61]\ttrain-mlogloss:1.20575\teval-mlogloss:1.26415\n",
      "[62]\ttrain-mlogloss:1.20338\teval-mlogloss:1.26270\n",
      "[63]\ttrain-mlogloss:1.20112\teval-mlogloss:1.26146\n",
      "[64]\ttrain-mlogloss:1.19893\teval-mlogloss:1.26014\n",
      "[65]\ttrain-mlogloss:1.19666\teval-mlogloss:1.25886\n",
      "[66]\ttrain-mlogloss:1.19448\teval-mlogloss:1.25757\n",
      "[67]\ttrain-mlogloss:1.19232\teval-mlogloss:1.25624\n",
      "[68]\ttrain-mlogloss:1.19012\teval-mlogloss:1.25491\n",
      "[69]\ttrain-mlogloss:1.18799\teval-mlogloss:1.25363\n",
      "[70]\ttrain-mlogloss:1.18582\teval-mlogloss:1.25245\n",
      "[71]\ttrain-mlogloss:1.18360\teval-mlogloss:1.25118\n",
      "[72]\ttrain-mlogloss:1.18151\teval-mlogloss:1.24999\n",
      "[73]\ttrain-mlogloss:1.17942\teval-mlogloss:1.24884\n",
      "[74]\ttrain-mlogloss:1.17737\teval-mlogloss:1.24767\n",
      "[75]\ttrain-mlogloss:1.17531\teval-mlogloss:1.24655\n",
      "[76]\ttrain-mlogloss:1.17322\teval-mlogloss:1.24546\n",
      "[77]\ttrain-mlogloss:1.17111\teval-mlogloss:1.24432\n",
      "[78]\ttrain-mlogloss:1.16910\teval-mlogloss:1.24320\n",
      "[79]\ttrain-mlogloss:1.16708\teval-mlogloss:1.24202\n",
      "[80]\ttrain-mlogloss:1.16505\teval-mlogloss:1.24086\n",
      "[81]\ttrain-mlogloss:1.16298\teval-mlogloss:1.23967\n",
      "[82]\ttrain-mlogloss:1.16102\teval-mlogloss:1.23860\n",
      "[83]\ttrain-mlogloss:1.15902\teval-mlogloss:1.23754\n",
      "[84]\ttrain-mlogloss:1.15710\teval-mlogloss:1.23652\n",
      "[85]\ttrain-mlogloss:1.15522\teval-mlogloss:1.23543\n",
      "[86]\ttrain-mlogloss:1.15339\teval-mlogloss:1.23449\n",
      "[87]\ttrain-mlogloss:1.15148\teval-mlogloss:1.23327\n",
      "[88]\ttrain-mlogloss:1.14951\teval-mlogloss:1.23220\n",
      "[89]\ttrain-mlogloss:1.14767\teval-mlogloss:1.23131\n",
      "[90]\ttrain-mlogloss:1.14583\teval-mlogloss:1.23037\n",
      "[91]\ttrain-mlogloss:1.14387\teval-mlogloss:1.22938\n",
      "[92]\ttrain-mlogloss:1.14197\teval-mlogloss:1.22839\n",
      "[93]\ttrain-mlogloss:1.14014\teval-mlogloss:1.22738\n",
      "[94]\ttrain-mlogloss:1.13829\teval-mlogloss:1.22646\n",
      "[95]\ttrain-mlogloss:1.13647\teval-mlogloss:1.22548\n",
      "[96]\ttrain-mlogloss:1.13455\teval-mlogloss:1.22454\n",
      "[97]\ttrain-mlogloss:1.13288\teval-mlogloss:1.22364\n",
      "[98]\ttrain-mlogloss:1.13109\teval-mlogloss:1.22259\n",
      "[99]\ttrain-mlogloss:1.12921\teval-mlogloss:1.22161\n",
      "[100]\ttrain-mlogloss:1.12750\teval-mlogloss:1.22079\n",
      "[101]\ttrain-mlogloss:1.12567\teval-mlogloss:1.21988\n",
      "[102]\ttrain-mlogloss:1.12393\teval-mlogloss:1.21901\n",
      "[103]\ttrain-mlogloss:1.12222\teval-mlogloss:1.21813\n",
      "[104]\ttrain-mlogloss:1.12046\teval-mlogloss:1.21731\n",
      "[105]\ttrain-mlogloss:1.11877\teval-mlogloss:1.21644\n",
      "[106]\ttrain-mlogloss:1.11712\teval-mlogloss:1.21559\n",
      "[107]\ttrain-mlogloss:1.11537\teval-mlogloss:1.21472\n",
      "[108]\ttrain-mlogloss:1.11353\teval-mlogloss:1.21388\n",
      "[109]\ttrain-mlogloss:1.11173\teval-mlogloss:1.21304\n",
      "[110]\ttrain-mlogloss:1.11005\teval-mlogloss:1.21225\n",
      "[111]\ttrain-mlogloss:1.10837\teval-mlogloss:1.21133\n",
      "[112]\ttrain-mlogloss:1.10665\teval-mlogloss:1.21052\n",
      "[113]\ttrain-mlogloss:1.10504\teval-mlogloss:1.20975\n",
      "[114]\ttrain-mlogloss:1.10348\teval-mlogloss:1.20897\n",
      "[115]\ttrain-mlogloss:1.10193\teval-mlogloss:1.20822\n",
      "[116]\ttrain-mlogloss:1.10035\teval-mlogloss:1.20740\n",
      "[117]\ttrain-mlogloss:1.09879\teval-mlogloss:1.20667\n",
      "[118]\ttrain-mlogloss:1.09726\teval-mlogloss:1.20591\n",
      "[119]\ttrain-mlogloss:1.09575\teval-mlogloss:1.20514\n",
      "[120]\ttrain-mlogloss:1.09410\teval-mlogloss:1.20431\n",
      "[121]\ttrain-mlogloss:1.09251\teval-mlogloss:1.20362\n",
      "[122]\ttrain-mlogloss:1.09098\teval-mlogloss:1.20292\n",
      "[123]\ttrain-mlogloss:1.08941\teval-mlogloss:1.20222\n",
      "[124]\ttrain-mlogloss:1.08793\teval-mlogloss:1.20152\n",
      "[125]\ttrain-mlogloss:1.08629\teval-mlogloss:1.20076\n",
      "[126]\ttrain-mlogloss:1.08476\teval-mlogloss:1.20004\n",
      "[127]\ttrain-mlogloss:1.08320\teval-mlogloss:1.19936\n",
      "[128]\ttrain-mlogloss:1.08158\teval-mlogloss:1.19883\n",
      "[129]\ttrain-mlogloss:1.08007\teval-mlogloss:1.19807\n",
      "[130]\ttrain-mlogloss:1.07854\teval-mlogloss:1.19740\n",
      "[131]\ttrain-mlogloss:1.07700\teval-mlogloss:1.19671\n",
      "[132]\ttrain-mlogloss:1.07544\teval-mlogloss:1.19600\n",
      "[133]\ttrain-mlogloss:1.07396\teval-mlogloss:1.19541\n",
      "[134]\ttrain-mlogloss:1.07248\teval-mlogloss:1.19471\n",
      "[135]\ttrain-mlogloss:1.07098\teval-mlogloss:1.19402\n",
      "[136]\ttrain-mlogloss:1.06956\teval-mlogloss:1.19341\n",
      "[137]\ttrain-mlogloss:1.06817\teval-mlogloss:1.19291\n",
      "[138]\ttrain-mlogloss:1.06672\teval-mlogloss:1.19224\n",
      "[139]\ttrain-mlogloss:1.06527\teval-mlogloss:1.19164\n",
      "[140]\ttrain-mlogloss:1.06375\teval-mlogloss:1.19099\n",
      "[141]\ttrain-mlogloss:1.06244\teval-mlogloss:1.19040\n",
      "[142]\ttrain-mlogloss:1.06103\teval-mlogloss:1.18977\n",
      "[143]\ttrain-mlogloss:1.05963\teval-mlogloss:1.18919\n",
      "[144]\ttrain-mlogloss:1.05836\teval-mlogloss:1.18860\n",
      "[145]\ttrain-mlogloss:1.05693\teval-mlogloss:1.18805\n",
      "[146]\ttrain-mlogloss:1.05544\teval-mlogloss:1.18746\n",
      "[147]\ttrain-mlogloss:1.05407\teval-mlogloss:1.18697\n",
      "[148]\ttrain-mlogloss:1.05265\teval-mlogloss:1.18639\n",
      "[149]\ttrain-mlogloss:1.05128\teval-mlogloss:1.18591\n",
      "[150]\ttrain-mlogloss:1.04979\teval-mlogloss:1.18544\n",
      "[151]\ttrain-mlogloss:1.04850\teval-mlogloss:1.18483\n",
      "[152]\ttrain-mlogloss:1.04701\teval-mlogloss:1.18435\n",
      "[153]\ttrain-mlogloss:1.04572\teval-mlogloss:1.18385\n",
      "[154]\ttrain-mlogloss:1.04432\teval-mlogloss:1.18334\n",
      "[155]\ttrain-mlogloss:1.04296\teval-mlogloss:1.18271\n",
      "[156]\ttrain-mlogloss:1.04159\teval-mlogloss:1.18220\n",
      "[157]\ttrain-mlogloss:1.04022\teval-mlogloss:1.18161\n",
      "[158]\ttrain-mlogloss:1.03896\teval-mlogloss:1.18116\n",
      "[159]\ttrain-mlogloss:1.03760\teval-mlogloss:1.18071\n",
      "[160]\ttrain-mlogloss:1.03628\teval-mlogloss:1.18019\n",
      "[161]\ttrain-mlogloss:1.03496\teval-mlogloss:1.17975\n",
      "[162]\ttrain-mlogloss:1.03358\teval-mlogloss:1.17924\n",
      "[163]\ttrain-mlogloss:1.03227\teval-mlogloss:1.17871\n",
      "[164]\ttrain-mlogloss:1.03102\teval-mlogloss:1.17832\n",
      "[165]\ttrain-mlogloss:1.02967\teval-mlogloss:1.17778\n",
      "[166]\ttrain-mlogloss:1.02844\teval-mlogloss:1.17731\n",
      "[167]\ttrain-mlogloss:1.02717\teval-mlogloss:1.17689\n",
      "[168]\ttrain-mlogloss:1.02598\teval-mlogloss:1.17644\n",
      "[169]\ttrain-mlogloss:1.02476\teval-mlogloss:1.17603\n",
      "[170]\ttrain-mlogloss:1.02351\teval-mlogloss:1.17557\n",
      "[171]\ttrain-mlogloss:1.02233\teval-mlogloss:1.17516\n",
      "[172]\ttrain-mlogloss:1.02106\teval-mlogloss:1.17470\n",
      "[173]\ttrain-mlogloss:1.01982\teval-mlogloss:1.17439\n",
      "[174]\ttrain-mlogloss:1.01850\teval-mlogloss:1.17386\n",
      "[175]\ttrain-mlogloss:1.01729\teval-mlogloss:1.17340\n",
      "[176]\ttrain-mlogloss:1.01610\teval-mlogloss:1.17298\n",
      "[177]\ttrain-mlogloss:1.01493\teval-mlogloss:1.17259\n",
      "[178]\ttrain-mlogloss:1.01365\teval-mlogloss:1.17210\n",
      "[179]\ttrain-mlogloss:1.01239\teval-mlogloss:1.17157\n",
      "[180]\ttrain-mlogloss:1.01112\teval-mlogloss:1.17118\n",
      "[181]\ttrain-mlogloss:1.00998\teval-mlogloss:1.17074\n",
      "[182]\ttrain-mlogloss:1.00869\teval-mlogloss:1.17029\n",
      "[183]\ttrain-mlogloss:1.00751\teval-mlogloss:1.16990\n",
      "[184]\ttrain-mlogloss:1.00626\teval-mlogloss:1.16951\n",
      "[185]\ttrain-mlogloss:1.00507\teval-mlogloss:1.16917\n",
      "[186]\ttrain-mlogloss:1.00394\teval-mlogloss:1.16885\n",
      "[187]\ttrain-mlogloss:1.00267\teval-mlogloss:1.16843\n",
      "[188]\ttrain-mlogloss:1.00161\teval-mlogloss:1.16798\n",
      "[189]\ttrain-mlogloss:1.00043\teval-mlogloss:1.16766\n",
      "[190]\ttrain-mlogloss:0.99925\teval-mlogloss:1.16729\n",
      "[191]\ttrain-mlogloss:0.99808\teval-mlogloss:1.16690\n",
      "[192]\ttrain-mlogloss:0.99688\teval-mlogloss:1.16643\n",
      "[193]\ttrain-mlogloss:0.99582\teval-mlogloss:1.16606\n",
      "[194]\ttrain-mlogloss:0.99452\teval-mlogloss:1.16571\n",
      "[195]\ttrain-mlogloss:0.99352\teval-mlogloss:1.16523\n",
      "[196]\ttrain-mlogloss:0.99245\teval-mlogloss:1.16493\n",
      "[197]\ttrain-mlogloss:0.99139\teval-mlogloss:1.16462\n",
      "[198]\ttrain-mlogloss:0.99042\teval-mlogloss:1.16424\n",
      "[199]\ttrain-mlogloss:0.98936\teval-mlogloss:1.16382\n",
      "[200]\ttrain-mlogloss:0.98829\teval-mlogloss:1.16348\n",
      "[201]\ttrain-mlogloss:0.98735\teval-mlogloss:1.16312\n",
      "[202]\ttrain-mlogloss:0.98619\teval-mlogloss:1.16269\n",
      "[203]\ttrain-mlogloss:0.98506\teval-mlogloss:1.16234\n",
      "[204]\ttrain-mlogloss:0.98404\teval-mlogloss:1.16207\n",
      "[205]\ttrain-mlogloss:0.98295\teval-mlogloss:1.16175\n",
      "[206]\ttrain-mlogloss:0.98196\teval-mlogloss:1.16137\n",
      "[207]\ttrain-mlogloss:0.98074\teval-mlogloss:1.16101\n",
      "[208]\ttrain-mlogloss:0.97970\teval-mlogloss:1.16069\n",
      "[209]\ttrain-mlogloss:0.97851\teval-mlogloss:1.16042\n",
      "[210]\ttrain-mlogloss:0.97745\teval-mlogloss:1.16011\n",
      "[211]\ttrain-mlogloss:0.97642\teval-mlogloss:1.15979\n",
      "[212]\ttrain-mlogloss:0.97526\teval-mlogloss:1.15945\n",
      "[213]\ttrain-mlogloss:0.97415\teval-mlogloss:1.15912\n",
      "[214]\ttrain-mlogloss:0.97307\teval-mlogloss:1.15881\n",
      "[215]\ttrain-mlogloss:0.97191\teval-mlogloss:1.15841\n",
      "[216]\ttrain-mlogloss:0.97073\teval-mlogloss:1.15804\n",
      "[217]\ttrain-mlogloss:0.96955\teval-mlogloss:1.15777\n",
      "[218]\ttrain-mlogloss:0.96858\teval-mlogloss:1.15753\n",
      "[219]\ttrain-mlogloss:0.96751\teval-mlogloss:1.15722\n",
      "[220]\ttrain-mlogloss:0.96636\teval-mlogloss:1.15700\n",
      "[221]\ttrain-mlogloss:0.96535\teval-mlogloss:1.15675\n",
      "[222]\ttrain-mlogloss:0.96440\teval-mlogloss:1.15652\n",
      "[223]\ttrain-mlogloss:0.96339\teval-mlogloss:1.15625\n",
      "[224]\ttrain-mlogloss:0.96245\teval-mlogloss:1.15603\n",
      "[225]\ttrain-mlogloss:0.96137\teval-mlogloss:1.15574\n",
      "[226]\ttrain-mlogloss:0.96027\teval-mlogloss:1.15544\n",
      "[227]\ttrain-mlogloss:0.95931\teval-mlogloss:1.15524\n",
      "[228]\ttrain-mlogloss:0.95826\teval-mlogloss:1.15498\n",
      "[229]\ttrain-mlogloss:0.95720\teval-mlogloss:1.15465\n",
      "[230]\ttrain-mlogloss:0.95629\teval-mlogloss:1.15433\n",
      "[231]\ttrain-mlogloss:0.95524\teval-mlogloss:1.15407\n",
      "[232]\ttrain-mlogloss:0.95423\teval-mlogloss:1.15378\n",
      "[233]\ttrain-mlogloss:0.95316\teval-mlogloss:1.15349\n",
      "[234]\ttrain-mlogloss:0.95222\teval-mlogloss:1.15319\n",
      "[235]\ttrain-mlogloss:0.95138\teval-mlogloss:1.15301\n",
      "[236]\ttrain-mlogloss:0.95040\teval-mlogloss:1.15280\n",
      "[237]\ttrain-mlogloss:0.94932\teval-mlogloss:1.15250\n",
      "[238]\ttrain-mlogloss:0.94829\teval-mlogloss:1.15215\n",
      "[239]\ttrain-mlogloss:0.94736\teval-mlogloss:1.15190\n",
      "[240]\ttrain-mlogloss:0.94648\teval-mlogloss:1.15156\n",
      "[241]\ttrain-mlogloss:0.94537\teval-mlogloss:1.15133\n",
      "[242]\ttrain-mlogloss:0.94436\teval-mlogloss:1.15113\n",
      "[243]\ttrain-mlogloss:0.94343\teval-mlogloss:1.15089\n",
      "[244]\ttrain-mlogloss:0.94242\teval-mlogloss:1.15060\n",
      "[245]\ttrain-mlogloss:0.94154\teval-mlogloss:1.15041\n",
      "[246]\ttrain-mlogloss:0.94064\teval-mlogloss:1.15019\n",
      "[247]\ttrain-mlogloss:0.93965\teval-mlogloss:1.14998\n",
      "[248]\ttrain-mlogloss:0.93880\teval-mlogloss:1.14974\n",
      "[249]\ttrain-mlogloss:0.93774\teval-mlogloss:1.14946\n",
      "[250]\ttrain-mlogloss:0.93674\teval-mlogloss:1.14922\n",
      "[251]\ttrain-mlogloss:0.93585\teval-mlogloss:1.14894\n",
      "[252]\ttrain-mlogloss:0.93480\teval-mlogloss:1.14877\n",
      "[253]\ttrain-mlogloss:0.93379\teval-mlogloss:1.14849\n",
      "[254]\ttrain-mlogloss:0.93274\teval-mlogloss:1.14832\n",
      "[255]\ttrain-mlogloss:0.93185\teval-mlogloss:1.14814\n",
      "[256]\ttrain-mlogloss:0.93092\teval-mlogloss:1.14792\n",
      "[257]\ttrain-mlogloss:0.93000\teval-mlogloss:1.14767\n",
      "[258]\ttrain-mlogloss:0.92905\teval-mlogloss:1.14749\n",
      "[259]\ttrain-mlogloss:0.92809\teval-mlogloss:1.14736\n",
      "[260]\ttrain-mlogloss:0.92726\teval-mlogloss:1.14718\n",
      "[261]\ttrain-mlogloss:0.92629\teval-mlogloss:1.14701\n",
      "[262]\ttrain-mlogloss:0.92532\teval-mlogloss:1.14677\n",
      "[263]\ttrain-mlogloss:0.92445\teval-mlogloss:1.14656\n",
      "[264]\ttrain-mlogloss:0.92362\teval-mlogloss:1.14637\n",
      "[265]\ttrain-mlogloss:0.92283\teval-mlogloss:1.14614\n",
      "[266]\ttrain-mlogloss:0.92193\teval-mlogloss:1.14588\n",
      "[267]\ttrain-mlogloss:0.92115\teval-mlogloss:1.14568\n",
      "[268]\ttrain-mlogloss:0.92025\teval-mlogloss:1.14546\n",
      "[269]\ttrain-mlogloss:0.91936\teval-mlogloss:1.14535\n",
      "[270]\ttrain-mlogloss:0.91867\teval-mlogloss:1.14519\n",
      "[271]\ttrain-mlogloss:0.91796\teval-mlogloss:1.14501\n",
      "[272]\ttrain-mlogloss:0.91705\teval-mlogloss:1.14482\n",
      "[273]\ttrain-mlogloss:0.91607\teval-mlogloss:1.14468\n",
      "[274]\ttrain-mlogloss:0.91530\teval-mlogloss:1.14456\n",
      "훈련 정확도: 0.779722079258878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.34      0.51      1000\n",
      "           1       0.78      0.85      0.81      3544\n",
      "           2       0.73      0.91      0.81      3671\n",
      "           3       0.90      0.60      0.72      1500\n",
      "\n",
      "    accuracy                           0.78      9715\n",
      "   macro avg       0.85      0.67      0.71      9715\n",
      "weighted avg       0.80      0.78      0.77      9715\n",
      "\n",
      "검증 정확도: 0.4452054794520548\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.02      0.04       149\n",
      "           1       0.43      0.47      0.45       678\n",
      "           2       0.48      0.58      0.52       745\n",
      "           3       0.30      0.15      0.20       180\n",
      "\n",
      "    accuracy                           0.45      1752\n",
      "   macro avg       0.34      0.30      0.30      1752\n",
      "weighted avg       0.41      0.45      0.42      1752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 제준님이 주신 새로운 피쳐들 사용 (하이퍼 파라미터 적용)\n",
    "\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = new_df2.loc[new_df2[\"_type\"] == \"train\"].drop(columns=[\"_type\"])\n",
    "train_df = train_df.ffill()\n",
    "test_df = new_df2.loc[new_df2[\"_type\"] == \"test\"].drop(columns=[\"_type\"])\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis=1),\n",
    "    train_df[\"target\"].astype(int),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# SMOTE를 이용한 오버샘플링\n",
    "sampling_strategy = {0: 1000, 1: 3544, 2: 3671, 3: 1500}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "# XGBoost 모델을 위한 DMatrix로 변환\n",
    "dtrain = xgb.DMatrix(x_train_resampled, label=y_train_resampled)\n",
    "dvalid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "# XGBoost 모델 학습 파라미터\n",
    "params = {\n",
    "    \"objective\": \"multi:softprob\",  # 다중 클래스 분류\n",
    "    \"num_class\": len(y_train_resampled.unique()),  # 클래스 개수\n",
    "    \"eval_metric\": \"mlogloss\",  # 손실 함수 (멀티클래스 로지스틱 손실)\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# 교차 검증 설정 (5-fold)\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=275,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=10,\n",
    "    metrics=\"mlogloss\",\n",
    "    as_pandas=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 최적의 부스팅 라운드 수 확인\n",
    "best_num_boost_rounds = len(cv_results)\n",
    "\n",
    "# 교차 검증 결과 출력\n",
    "print(\"최적 부스팅 라운드 수:\", best_num_boost_rounds)\n",
    "print(cv_results.tail())\n",
    "\n",
    "# 최적의 부스팅 라운드로 모델 학습\n",
    "xgb_model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=best_num_boost_rounds,\n",
    "    evals=[(dtrain, \"train\"), (dvalid, \"eval\")],\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# 예측 (훈련 데이터셋)\n",
    "y_train_pred_prob = xgb_model.predict(dtrain)\n",
    "y_train_pred = y_train_pred_prob.argmax(axis=1)\n",
    "\n",
    "# 예측 (검증 데이터셋)\n",
    "y_valid_pred_prob = xgb_model.predict(dvalid)\n",
    "y_valid_pred = y_valid_pred_prob.argmax(axis=1)\n",
    "\n",
    "# 성능 평가 (훈련 데이터셋)\n",
    "train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "train_classification_rep = classification_report(y_train_resampled, y_train_pred)\n",
    "\n",
    "# 성능 평가 (검증 데이터셋)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "valid_classification_rep = classification_report(y_valid, y_valid_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"훈련 정확도: {train_accuracy}\")\n",
    "print(train_classification_rep)\n",
    "print(f\"검증 정확도: {valid_accuracy}\")\n",
    "print(valid_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# _type에 따라 train, test 분리\n",
    "train_df = new_df2.loc[new_df2[\"_type\"] == \"train\"].drop(columns=[\"_type\"])\n",
    "train_df = train_df.ffill().dropna()\n",
    "test_df = new_df2.loc[new_df2[\"_type\"] == \"test\"].drop(columns=[\"_type\"])\n",
    "\n",
    "# train_test_split 으로 valid set, train set 분리\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    train_df.drop([\"target\", \"ID\"], axis=1),\n",
    "    train_df[\"target\"].astype(int),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# SMOTE를 이용한 오버샘플링\n",
    "sampling_strategy = {0: 1000, 1: 3544, 2: 3671, 3: 1500}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "x_train_resampled, y_train_resampled = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 9715, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008415\n",
      "[LightGBM] [Info] Start training from score -0.973207\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "훈련 정확도: 0.9418425115800309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.72      0.84      1000\n",
      "           1       0.94      0.99      0.97      3544\n",
      "           2       0.91      0.99      0.95      3671\n",
      "           3       0.99      0.84      0.91      1500\n",
      "\n",
      "    accuracy                           0.94      9715\n",
      "   macro avg       0.96      0.89      0.92      9715\n",
      "weighted avg       0.95      0.94      0.94      9715\n",
      "\n",
      "검증 정확도: 0.4383561643835616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.01      0.03       136\n",
      "           1       0.41      0.48      0.44       668\n",
      "           2       0.48      0.55      0.51       783\n",
      "           3       0.27      0.11      0.16       165\n",
      "\n",
      "    accuracy                           0.44      1752\n",
      "   macro avg       0.32      0.29      0.28      1752\n",
      "weighted avg       0.41      0.44      0.41      1752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# XGBoost 모델 (하이퍼파라미터는 그대로 사용)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",  # 다중 클래스 분류\n",
    "    num_class=len(y_train_resampled.unique()),  # 클래스 개수\n",
    "    eval_metric=\"mlogloss\",  # 손실 함수 (멀티클래스 로지스틱 손실)\n",
    "    max_depth=7,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 추가 모델들 (로지스틱 회귀 및 랜덤 포레스트)\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model = LGBMClassifier()\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# Voting Classifier 앙상블 (하드 보팅: voting='hard' 또는 소프트 보팅: voting='soft')\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('log', log_model),\n",
    "        ('rf', rf_model),\n",
    "        ('lgb',lgb_model),\n",
    "        ('gbm',gbm_model),\n",
    "        ('svm',svm_model),\n",
    "    ],\n",
    "    voting='soft'  # 확률을 기반으로 투표\n",
    ")\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "ensemble_model.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 (훈련 데이터셋)\n",
    "y_train_pred = ensemble_model.predict(x_train_resampled)\n",
    "\n",
    "# 예측 (검증 데이터셋)\n",
    "y_valid_pred = ensemble_model.predict(x_valid)\n",
    "\n",
    "# 성능 평가 (훈련 데이터셋)\n",
    "train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "train_classification_rep = classification_report(y_train_resampled, y_train_pred)\n",
    "\n",
    "# 성능 평가 (검증 데이터셋)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "valid_classification_rep = classification_report(y_valid, y_valid_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"훈련 정확도: {train_accuracy}\")\n",
    "print(train_classification_rep)\n",
    "print(f\"검증 정확도: {valid_accuracy}\")\n",
    "print(valid_classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VotingClassifier.predict() missing 1 required positional argument: 'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 테스트 데이터에 대한 예측 수행\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_test_pred_prob \u001b[38;5;241m=\u001b[39m VotingClassifier\u001b[38;5;241m.\u001b[39mpredict(test_df)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 다중 클래스 예측일 경우 가장 높은 확률을 가진 클래스를 예측값으로 변환\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m y_test_pred_prob\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: VotingClassifier.predict() missing 1 required positional argument: 'X'"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 예측 수행\n",
    "y_test_pred_prob = VotingClassifier.predict(test_df)\n",
    "\n",
    "# 다중 클래스 예측일 경우 가장 높은 확률을 가진 클래스를 예측값으로 변환\n",
    "y_test_pred = y_test_pred_prob.argmax(axis=1)\n",
    "\n",
    "# 예측 결과 출력\n",
    "y_test_pred\n",
    "\n",
    "submission_df = submission_df.assign(target=pd.DataFrame(y_test_pred))\n",
    "submission_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 9715, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008415\n",
      "[LightGBM] [Info] Start training from score -0.973207\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008133\n",
      "[LightGBM] [Info] Start training from score -0.973479\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008486\n",
      "[LightGBM] [Info] Start training from score -0.973139\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008486\n",
      "[LightGBM] [Info] Start training from score -0.973139\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008486\n",
      "[LightGBM] [Info] Start training from score -0.973139\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4335\n",
      "[LightGBM] [Info] Number of data points in the train set: 7772, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score -2.273671\n",
      "[LightGBM] [Info] Start training from score -1.008486\n",
      "[LightGBM] [Info] Start training from score -0.973139\n",
      "[LightGBM] [Info] Start training from score -1.868206\n",
      "훈련 정확도: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1000\n",
      "           1       1.00      1.00      1.00      3544\n",
      "           2       1.00      1.00      1.00      3671\n",
      "           3       1.00      1.00      1.00      1500\n",
      "\n",
      "    accuracy                           1.00      9715\n",
      "   macro avg       1.00      1.00      1.00      9715\n",
      "weighted avg       1.00      1.00      1.00      9715\n",
      "\n",
      "검증 정확도: 0.420662100456621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.05      0.07       136\n",
      "           1       0.41      0.49      0.45       668\n",
      "           2       0.48      0.48      0.48       783\n",
      "           3       0.21      0.13      0.16       165\n",
      "\n",
      "    accuracy                           0.42      1752\n",
      "   macro avg       0.30      0.29      0.29      1752\n",
      "weighted avg       0.40      0.42      0.41      1752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# XGBoost 모델 (하이퍼파라미터는 그대로 사용)\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"multi:softprob\",  # 다중 클래스 분류\n",
    "    num_class=len(y_train_resampled.unique()),  # 클래스 개수\n",
    "    eval_metric=\"mlogloss\",  # 손실 함수 (멀티클래스 로지스틱 손실)\n",
    "    max_depth=7,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# 추가 모델들 (로지스틱 회귀 및 랜덤 포레스트)\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lgb_model = LGBMClassifier()\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "svm_model = SVC(probability=True)\n",
    "\n",
    "# 스태킹 앙상블 모델 (최종 메타 모델로 로지스틱 회귀 사용)\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_model),\n",
    "        ('log', log_model),\n",
    "        ('rf', rf_model),\n",
    "        ('lgb',lgb_model),\n",
    "        ('gbm',gbm_model),\n",
    "        ('svm',svm_model),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression()  # 메타 모델\n",
    ")\n",
    "\n",
    "# 앙상블 모델 학습\n",
    "stacking_model.fit(x_train_resampled, y_train_resampled)\n",
    "\n",
    "# 예측 (훈련 데이터셋)\n",
    "y_train_pred = stacking_model.predict(x_train_resampled)\n",
    "\n",
    "# 예측 (검증 데이터셋)\n",
    "y_valid_pred = stacking_model.predict(x_valid)\n",
    "\n",
    "# 성능 평가 (훈련 데이터셋)\n",
    "train_accuracy = accuracy_score(y_train_resampled, y_train_pred)\n",
    "train_classification_rep = classification_report(y_train_resampled, y_train_pred)\n",
    "\n",
    "# 성능 평가 (검증 데이터셋)\n",
    "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "valid_classification_rep = classification_report(y_valid, y_valid_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"훈련 정확도: {train_accuracy}\")\n",
    "print(train_classification_rep)\n",
    "print(f\"검증 정확도: {valid_accuracy}\")\n",
    "print(valid_classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "2    1815\n",
       "1     789\n",
       "3     128\n",
       "0      60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터셋을 DMatrix로 변환\n",
    "dtest = xgb.DMatrix(test_df.drop([\"target\", \"ID\"], axis=1))\n",
    "\n",
    "# 테스트 데이터에 대한 예측 수행\n",
    "y_test_pred_prob = xgb_model.predict(dtest)\n",
    "\n",
    "# 다중 클래스 예측일 경우 가장 높은 확률을 가진 클래스를 예측값으로 변환\n",
    "y_test_pred = y_test_pred_prob.argmax(axis=1)\n",
    "\n",
    "# 예측 결과 출력\n",
    "y_test_pred\n",
    "\n",
    "submission_df = submission_df.assign(target=pd.DataFrame(y_test_pred))\n",
    "submission_df['target'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
